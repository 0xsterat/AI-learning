{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbee0e43",
   "metadata": {},
   "source": [
    "# Context Window: Why Chatbots Forget\n",
    "Every model has a context window limit - the maximum number of tokens it can process in a single request.\n",
    "\n",
    "This includes input + output tokens combined.\n",
    "\n",
    "Let's see what happens when you exceed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c304419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f35e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Story length: 5,900,000 characters\n",
      "ğŸ“ Estimated tokens: ~1,475,000\n",
      "\n",
      "âŒ ERROR: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 250000, model: gemini-2.5-flash-lite\\nPlease retry in 19.061927089s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '250000'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '19s'}]}}\n",
      "\n",
      "ğŸ’¡ This is what happens when you exceed the context window!\n"
     ]
    }
   ],
   "source": [
    "# Create a VERY long message to hit the limit\n",
    "long_story = \"Once upon a time, there was a programmer who loved Python. \" * 100000\n",
    "\n",
    "print(f\"ğŸ“ Story length: {len(long_story):,} characters\")\n",
    "print(f\"ğŸ“ Estimated tokens: ~{len(long_story) // 4:,}\\n\")\n",
    "\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=long_story,\n",
    "        config={\"max_output_tokens\": 100}\n",
    "    )\n",
    "    print(\"âœ… Request succeeded!\")\n",
    "    print(f\"ğŸ“Š Tokens used: {response.usage_metadata.total_token_count:,}\")\n",
    "    print(f\"\\nğŸ’¡ Gemini 2.5 Flash has a HUGE context window (~1M tokens)\")\n",
    "    print(f\"   So this request fits comfortably!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: {e}\")\n",
    "    print(\"\\nğŸ’¡ This is what happens when you exceed the context window!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b4a93",
   "metadata": {},
   "source": [
    "ğŸ’¡ This is what happens when you exceed the context window!\n",
    "# What is Context Window?\n",
    "\n",
    "## Context Window = Maximum tokens in a single request\n",
    "\n",
    "Different models have different limits:\n",
    "\n",
    "* Gemini 2.5 Flash: ~1 million tokens\n",
    "* GPT-4: ~128K tokens\n",
    "* Claude 4: ~200K tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87de960",
   "metadata": {},
   "source": [
    "# Context Window in Conversations\n",
    "In chat applications, the context window includes:\n",
    "\n",
    "* All previous messages (entire conversation history)\n",
    "* Current message\n",
    "* Response\n",
    "\n",
    "As conversations grow, tokens accumulate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f04fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤: Hi! My name is Atharva\n",
      "ğŸ¤–: Hi Atharva! It's nice to meet you. How can I help you today?\n",
      "ğŸ“Š Total tokens used: 29 (includes ALL 2 messages)\n",
      "\n",
      "ğŸ‘¤: I'm 25 years old\n",
      "ğŸ¤–: Thanks for letting me know, Atharva! That's a great age. What can I do for you today?\n",
      "ğŸ“Š Total tokens used: 64 (includes ALL 4 messages)\n",
      "\n",
      "ğŸ‘¤: I love Python programming\n",
      "ğŸ¤–: That's fantastic, Atharva! Python is a wonderfully versatile and popular programming language. I'm a big fan of it myself.\n",
      "\n",
      "What do you enjoy most about Python? Are you working on any specific projects, or are you interested in learning something new about it? I'm here to chat about anything related to Python!\n",
      "ğŸ“Š Total tokens used: 139 (includes ALL 6 messages)\n",
      "\n",
      "ğŸ‘¤: I work as a software engineer\n",
      "ğŸ¤–: That's excellent, Atharva! Working as a software engineer is a very rewarding and dynamic field. It's great that you're applying your passion for Python in your career.\n",
      "\n",
      "What kind of software do you typically work on? Are you in web development, data science, backend systems, or something else? I'm always interested to hear about what fellow engineers are building!\n",
      "ğŸ“Š Total tokens used: 227 (includes ALL 8 messages)\n",
      "\n",
      "ğŸ‘¤: What's my name?\n",
      "ğŸ¤–: Your name is **Atharva**.\n",
      "ğŸ“Š Total tokens used: 243 (includes ALL 10 messages)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is **Atharva**.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "def chat(user_message):\n",
    "    \"\"\"Send message and track tokens\"\"\"\n",
    "    messages.append(\n",
    "        types.Content(role=\"user\", parts=[types.Part(text=user_message)])\n",
    "    )\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=messages\n",
    "    )\n",
    "    \n",
    "    messages.append(\n",
    "        types.Content(role=\"model\", parts=[types.Part(text=response.text)])\n",
    "    )\n",
    "    \n",
    "    # Show token usage\n",
    "    total_tokens = response.usage_metadata.total_token_count\n",
    "    print(f\"ğŸ¤–: {response.text}\")\n",
    "    print(f\"ğŸ“Š Total tokens used: {total_tokens} (includes ALL {len(messages)} messages)\\n\")\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Start conversation\n",
    "print(\"ğŸ‘¤: Hi! My name is Atharva\")\n",
    "chat(\"Hi! My name is Atharva\")\n",
    "\n",
    "print(\"ğŸ‘¤: I'm 25 years old\")\n",
    "chat(\"I'm 25 years old\")\n",
    "\n",
    "print(\"ğŸ‘¤: I love Python programming\")\n",
    "chat(\"I love Python programming\")\n",
    "\n",
    "print(\"ğŸ‘¤: I work as a software engineer\")\n",
    "chat(\"I work as a software engineer\")\n",
    "\n",
    "print(\"ğŸ‘¤: What's my name?\")\n",
    "chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4666c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤: My favorite color is blue\n",
      "ğŸ¤–: That's a wonderful choice! Blue is such a versatile and beautiful color, often associated with peace, the sky, and the ocean.\n",
      "\n",
      "Do you have a favorite shade of blue, like navy, sky blue, sapphire, or perhaps a vibrant turquoise?\n",
      "ğŸ“Š Messages in memory: 2\n",
      "\n",
      "ğŸ‘¤: I have a dog named Max\n",
      "ğŸ¤–: Oh, how wonderful! Having a dog named Max sounds absolutely lovely! Max is such a classic and beloved dog name.\n",
      "\n",
      "Dogs bring so much joy and companionship to our lives. I'd love to hear more about him if you'd like to share!\n",
      "\n",
      "*   What kind of dog is Max?\n",
      "*   Is he a playful pup or more of a relaxed companion?\n",
      "*   How old is he?\n",
      "*   What are some of his favorite things to do?\n",
      "\n",
      "Tell me all about your good boy, Max!\n",
      "ğŸ“Š Messages in memory: 4\n",
      "\n",
      "ğŸ‘¤: I live in Mumbai\n",
      "ğŸ¤–: Mumbai! What an incredible and vibrant city. I've heard so much about its bustling energy, incredible food, rich history, and beautiful coastal views. It sounds like a fascinating place to live!\n",
      "\n",
      "Do you enjoy living there?\n",
      "ğŸ“Š Messages in memory: 6\n",
      "\n",
      "ğŸ‘¤: I enjoy hiking\n",
      "ğŸ—‘ï¸  Removed old message: My favorite color is blue...\n",
      "\n",
      "ğŸ¤–: That's a fantastic hobby to have! Hiking is such a wonderful way to connect with nature, get some exercise, and clear your head.\n",
      "\n",
      "Does your dog, Max, ever join you on your hikes? Many dogs absolutely love hitting the trails!\n",
      "ğŸ“Š Messages in memory: 7\n",
      "\n",
      "ğŸ‘¤: What's my favorite color?\n",
      "ğŸ—‘ï¸  Removed old message: That's a wonderful choice! Blue is such a versatil...\n",
      "\n",
      "ğŸ¤–: That's a fun question! As an AI, I don't actually know your personal preferences, including your favorite color. I don't have access to information about you beyond what you tell me in our conversation.\n",
      "\n",
      "So, you'll have to tell me! What *is* your favorite color? I'm curious!\n",
      "ğŸ“Š Messages in memory: 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a fun question! As an AI, I don't actually know your personal preferences, including your favorite color. I don't have access to information about you beyond what you tell me in our conversation.\\n\\nSo, you'll have to tell me! What *is* your favorite color? I'm curious!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_MESSAGES = 6  # Keep only last 6 messages (3 exchanges)\n",
    "\n",
    "def chat_with_limit(user_message):\n",
    "    \"\"\"Chat with context window management\"\"\"\n",
    "    messages.append(\n",
    "        types.Content(role=\"user\", parts=[types.Part(text=user_message)])\n",
    "    )\n",
    "    \n",
    "    # Remove old messages if too many\n",
    "    if len(messages) > MAX_MESSAGES:\n",
    "        removed = messages.pop(0)  # Remove oldest\n",
    "        print(f\"ğŸ—‘ï¸  Removed old message: {removed.parts[0].text[:50]}...\\n\")\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=messages\n",
    "    )\n",
    "    \n",
    "    messages.append(\n",
    "        types.Content(role=\"model\", parts=[types.Part(text=response.text)])\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ¤–: {response.text}\")\n",
    "    print(f\"ğŸ“Š Messages in memory: {len(messages)}\\n\")\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Reset and try again\n",
    "messages = []\n",
    "\n",
    "print(\"ğŸ‘¤: My favorite color is blue\")\n",
    "chat_with_limit(\"My favorite color is blue\")\n",
    "\n",
    "print(\"ğŸ‘¤: I have a dog named Max\")\n",
    "chat_with_limit(\"I have a dog named Max\")\n",
    "\n",
    "print(\"ğŸ‘¤: I live in Mumbai\")\n",
    "chat_with_limit(\"I live in Mumbai\")\n",
    "\n",
    "print(\"ğŸ‘¤: I enjoy hiking\")\n",
    "chat_with_limit(\"I enjoy hiking\")\n",
    "\n",
    "# This will forget the first message!\n",
    "print(\"ğŸ‘¤: What's my favorite color?\")\n",
    "chat_with_limit(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a860e",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "Context Window = Maximum tokens in a single request (input + output)\n",
    "\n",
    "In conversations, tokens grow with each message\n",
    "\n",
    "Must remove old messages when approaching limit\n",
    "\n",
    "This causes \"forgetting\" - AI loses old context\n",
    "\n",
    "Exceeding limit = Error - Request will fail\n",
    "\n",
    "## Real-World Solutions\n",
    "Summarize old messages instead of removing\n",
    "Store important info separately (database)\n",
    "Prioritize recent messages\n",
    "Use RAG (Retrieval Augmented Generation) for long-term memory\n",
    "Monitor token usage and manage proactively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
